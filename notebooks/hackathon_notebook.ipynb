{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weISGctwMQ_S"
      },
      "source": [
        "# Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "awR9q6qeNPlX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG']=':16:8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5CCSxXVZMDOQ"
      },
      "outputs": [],
      "source": [
        "seed = 66\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "np.random.RandomState(seed)\n",
        "\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "# import tensorflow as tf\n",
        "# tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwQz-wnFMc_z"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3jvk8EOjR0e_"
      },
      "outputs": [],
      "source": [
        "INT_TO_STR = {\n",
        "    0: 'descriptive',\n",
        "    1: 'direct',\n",
        "    2: 'non-offensive',\n",
        "    3: 'offensive',\n",
        "    4: 'reporting'\n",
        "}\n",
        "\n",
        "STR_TO_INT = {\n",
        "    'descriptive': 0,\n",
        "    'direct': 1,\n",
        "    'non-offensive': 2,\n",
        "    'offensive': 3,\n",
        "    'reporting': 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "exwODnFZMGiJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/dev-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100%|██████████| 1/1 [00:00<00:00, 594.01it/s]\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e21444dc684dec16/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100%|██████████| 1/1 [00:00<00:00, 759.01it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('csv', data_files={'data': '../data/train_data.csv'})\n",
        "ds_back = load_dataset('csv', data_files={'data': '../data/train_data_back.csv'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0rWnMjIDM2Ug"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "MODEL_CKPT = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLwnvBEaM9m0"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c80iN6O0MzV2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import emoji\n",
        "\n",
        "def normalize(batch):\n",
        "    \"\"\"\n",
        "    This function should be used before tokenizing the input string.\n",
        "\n",
        "    Normalizes the input string in the following ways:\n",
        "    -> Converts from ş to ș, ţ to ț, etc.\n",
        "    -> Converts @mention to USER, #hashtag to HASHTAG, http... and www... to HTTPURL\n",
        "    -> Converts emoticons to :emoji_with_long_name:\n",
        "    -> Replaces :emoji_with_long_name: with emoji_with_long_name and replaces _, : and - with empty string\n",
        "    -> Removes multiple whitespaces with a single whitespace\n",
        "    \"\"\"\n",
        "\n",
        "    sentence = batch['text']\n",
        "\n",
        "    # Make sure it's a string\n",
        "    sentence = str(sentence)\n",
        "\n",
        "    # Convert from ş to ș, ţ to ț, etc.\n",
        "    sentence = re.sub(r'ş', 'ș', sentence)\n",
        "    sentence = re.sub(r'Ş', 'Ș', sentence)\n",
        "    sentence = re.sub(r'ţ', 'ț', sentence)\n",
        "    sentence = re.sub(r'Ţ', 'Ț', sentence)\n",
        "\n",
        "    # Convert @mentions to USER, #hashtags to HASHTAG, http... and www... to HTTPURL\n",
        "    sentence = re.sub(r'@\\S+', 'USER', sentence)\n",
        "    sentence = re.sub(r'#\\S+', 'HASHTAG', sentence)\n",
        "    sentence = re.sub(r'http\\S+', 'HTTPURL', sentence)\n",
        "    sentence = re.sub(r'www\\S+', 'HTTPURL', sentence)\n",
        "\n",
        "    # Convert emoticons to :emoji_with_long_name:\n",
        "    sentence = emoji.demojize(sentence, delimiters=(' :', ': '))\n",
        "\n",
        "    # Replace :emoji_with_long_name: with emojiwithlongname\n",
        "    sentence = re.sub(r':\\S+:', lambda x: x.group(0).replace('_', '').replace(':', '').replace('-', ''), sentence)\n",
        "\n",
        "    # Remove multiple whitespaces with a single whitespace\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return {'text': sentence}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYEHAYmGNB_F"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ea-bD5YPM8j8"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i6MW5CHNbj8"
      },
      "source": [
        "## Torch format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIFoeBfRN05n"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ2SXUptOyPe"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SA_KOIvAO0yg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_metric, concatenate_datasets\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds  = pred.predictions.argmax(-1) # choose the predicted class (from an array of probabilites)\n",
        "\n",
        "    f1  = f1_score(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    bacc = balanced_accuracy_score(labels, preds)\n",
        "\n",
        "    return {'accuracy': acc, 'f1': f1, 'balanced_accuracy': bacc}\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        # loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([26.315, 18.181, 1.265, 9.090, 200.0]).to(device))\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([15.873, 11.111, 1.538, 5.555, 111.111]).to(device))#[, , , , ]))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def train(index: int, dataset_tokenized):\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_CKPT,\n",
        "        num_labels=5,\n",
        "        id2label=INT_TO_STR,\n",
        "        label2id=STR_TO_INT,\n",
        "        classifier_dropout=0.1,\n",
        "        # use_auth_token='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'\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"nitro-robertlarge-nlp-v1.9.{index}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=4,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "    \n",
        "    \n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_tokenized[\"train\"],\n",
        "        eval_dataset=dataset_tokenized[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    del trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100%|██████████| 1/1 [00:00<00:00, 680.12it/s]\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-bd226ea07604be46.arrow\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset('csv', data_files={'data': '../data/train_data.csv'})\n",
        "ds = ds.rename_column('Final Labels', 'label')\n",
        "ds = ds.rename_column('Text', 'text')\n",
        "ds = ds.remove_columns(['Id'])\n",
        "ds = ds['data']\n",
        "# ds = ds.class_encode_column('label')\n",
        "\n",
        "ds_back = ds_back.rename_column('text_back', 'text')\n",
        "ds_back = ds_back.rename_column('Final Labels', 'label')\n",
        "\n",
        "ds_back = ds_back.remove_columns(['Text', 'Id'])\n",
        "ds_back = ds_back['data']\n",
        "# ds_back = ds_back.class_encode_column('label')\n",
        "\n",
        "ds_concat = concatenate_datasets([ds, ds_back])\n",
        "ds_concat = ds_concat.class_encode_column('label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 47178\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0aed00f0af3897d4.arrow and /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0ee077293d016e55.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-58c5e4f1cd3f9786.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-754146f276ef539f.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-faa345e614eefe24.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-45bae733d7e469ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-621b50c92c36b019.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SEED:  66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/opt/dev-venv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='18872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [    3/18872 00:00 < 2:27:08, 2.14 it/s, Epoch 0.00/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def ensamble_train(index = 1, index_start = 0, index_end = 5):\n",
        "\n",
        "    seeds = []\n",
        "    for i in range(0, index):\n",
        "        seeds.append(seed + i)\n",
        "\n",
        "    for i in range(0, index):\n",
        "        if i >= index_start:\n",
        "            \n",
        "            print(\"SEED: \", seed)\n",
        "            ds_split = ds_concat.train_test_split(test_size=0.2, stratify_by_column='label', seed=seeds[i])\n",
        "            ds_split = ds_split.map(lambda batch: normalize(batch), batched=False)\n",
        "            ds_tok_split = ds_split.map(lambda batch: tokenize(batch), batched=True, batch_size=None)\n",
        "            ds_tok_split.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "            train(i, ds_tok_split)\n",
        "        else:\n",
        "            ds_split = ds.train_test_split(test_size=0.2, stratify_by_column='label', seed=seed)\n",
        "\n",
        "\n",
        "ensamble_train(1, 0, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "non-offensive    29296 (~79%)\n",
        "offensive         4086 (~11%)\n",
        "direct            2048 (~5.5%)\n",
        "descriptive       1419 (~3,8%)\n",
        "reporting          208 (~0.5%) 0.79k1 = 0.005k2 = 1\n",
        "Total: 37,057\n",
        "\n",
        "0: 'descriptive',\n",
        "1: 'direct',\n",
        "2: 'non-offensive',\n",
        "3: 'offensive',\n",
        "4: 'reporting'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnBOXVm7NPl2"
      },
      "source": [
        "# Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LtGAl1ieQW-Y"
      },
      "outputs": [],
      "source": [
        "def load_model(checkpoint_path: str, ds_tok):\n",
        "\n",
        "    model2 = AutoModelForSequenceClassification.from_pretrained(\n",
        "        checkpoint_path,\n",
        "        num_labels=5,\n",
        "        id2label=INT_TO_STR,\n",
        "        label2id=STR_TO_INT,\n",
        "        classifier_dropout=0.1,\n",
        "    )\n",
        "\n",
        "    training_args_ft = TrainingArguments(\n",
        "        output_dir=checkpoint_path,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=4,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    trainer2 = CustomTrainer(\n",
        "        model=model2,\n",
        "        args=training_args_ft,\n",
        "        train_dataset=ds_tok,\n",
        "        eval_dataset=ds_tok,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    return trainer2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AjQ7z2-oMGsC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-302d635d8fbd1d39\n",
            "Found cached dataset csv (C:/Users/andre/.cache/huggingface/datasets/csv/default-302d635d8fbd1d39/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "910c938e1bd44a7485c8e96971fc9bb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\andre\\.cache\\huggingface\\datasets\\csv\\default-302d635d8fbd1d39\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-1e84eeeb2d4ac9cd.arrow\n",
            "Loading cached processed dataset at C:\\Users\\andre\\.cache\\huggingface\\datasets\\csv\\default-302d635d8fbd1d39\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b9baf9b78a1a1263.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'În miezul ei se găsea un obiect ciudat , roz , răsucit , mărit de suprafața rotundă , care semăna cu un trandafir sau cu o anemonă de mare.'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_eval = load_dataset('csv', data_files={'data': './dataset/test_data.csv'})\n",
        "ds_eval = ds_eval.rename_column('Text', 'text')\n",
        "ds_eval = ds_eval.rename_column('Id', 'id')\n",
        "ds_eval_data = ds_eval['data']\n",
        "ds_eval_data = ds_eval_data.map(lambda batch: normalize(batch), batched=False)\n",
        "ds_eval_tok = ds_eval_data.map(lambda batch: tokenize(batch), batched=True, batch_size=None)\n",
        "ds_eval_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tUOuL5xJNPl3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file ./nitro-robertlarge-nlp-v1.9.0/checkpoint-3540\\config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"./nitro-robertlarge-nlp-v1.9.0/checkpoint-3540\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"descriptive\",\n",
            "    \"1\": \"direct\",\n",
            "    \"2\": \"non-offensive\",\n",
            "    \"3\": \"offensive\",\n",
            "    \"4\": \"reporting\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"descriptive\": 0,\n",
            "    \"direct\": 1,\n",
            "    \"non-offensive\": 2,\n",
            "    \"offensive\": 3,\n",
            "    \"reporting\": 4\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51000\n",
            "}\n",
            "\n",
            "loading weights file ./nitro-robertlarge-nlp-v1.9.0/checkpoint-3540\\pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./nitro-robertlarge-nlp-v1.9.0/checkpoint-3540.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 3130\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caf12c30274e40248eb4383840cd117c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/196 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[PredictionOutput(predictions=array([[-0.50763935, -1.3257251 ,  3.6892135 , -0.23153174, -2.728777  ],\n",
              "        [-2.887187  ,  0.7367201 ,  0.4814147 ,  3.2221    , -2.21238   ],\n",
              "        [-0.68719965,  0.08260232,  3.7809756 ,  1.0506375 , -4.01028   ],\n",
              "        ...,\n",
              "        [-0.48108423, -2.189384  ,  0.95113224,  4.044851  , -3.3308613 ],\n",
              "        [ 4.6631646 , -0.5120723 , -0.30559567, -0.8618112 , -2.926247  ],\n",
              "        [-3.2720423 ,  0.15021496,  2.0113256 ,  2.7746964 , -2.5090785 ]],\n",
              "       dtype=float32), label_ids=None, metrics={'test_runtime': 7.8423, 'test_samples_per_second': 399.116, 'test_steps_per_second': 24.993})]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensamble_list = [\n",
        "    './nitro-robertlarge-nlp-v1.9.0/checkpoint-3540'\n",
        "]\n",
        "\n",
        "ensamble_predictions = []\n",
        "for model in ensamble_list:\n",
        "    trainer = load_model(model, ds_eval_tok)\n",
        "\n",
        "    predictions = trainer.predict(ds_eval_tok)\n",
        "    ensamble_predictions.append(predictions)\n",
        "\n",
        "    del trainer\n",
        "\n",
        "ensamble_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "    return(np.exp(x)/np.exp(x).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_ensamble_prediction = ensamble_predictions[0].predictions\n",
        "\n",
        "for i in range(1, len(ensamble_predictions)):\n",
        "    print(i)\n",
        "    final_ensamble_prediction = final_ensamble_prediction + ensamble_predictions[i].predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EUPd_OSuNPl3"
      },
      "outputs": [],
      "source": [
        "preds = np.argmax(np.array(final_ensamble_prediction), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UjNtmS1DNPl3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3130,)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "38XxQCZWNPl3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({})\n",
        "\n",
        "with open(f'./subs/nitro-robertweet-nlp-v2.1.0.csv', 'w', newline='') as csvfile:\n",
        "    data = []\n",
        "    for i, pred in enumerate(preds):\n",
        "        data.append([i, INT_TO_STR[pred]])\n",
        "\n",
        "    header=['Id', 'Label']\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5f9qLKlNPl3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "myvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "efc8b837990332cc640cf04fb86e57a3faa233210bbd8a49efb74870320cb06c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
