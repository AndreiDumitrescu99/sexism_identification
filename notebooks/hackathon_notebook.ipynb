{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weISGctwMQ_S"
   },
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awR9q6qeNPlX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CCSxXVZMDOQ"
   },
   "outputs": [],
   "source": [
    "seed = 66\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwQz-wnFMc_z"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jvk8EOjR0e_"
   },
   "outputs": [],
   "source": [
    "INT_TO_STR = {0: \"descriptive\", 1: \"direct\", 2: \"non-offensive\", 3: \"offensive\", 4: \"reporting\"}\n",
    "\n",
    "STR_TO_INT = {\"descriptive\": 0, \"direct\": 1, \"non-offensive\": 2, \"offensive\": 3, \"reporting\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exwODnFZMGiJ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csv\", data_files={\"data\": \"../data/train_data.csv\"})\n",
    "ds_back = load_dataset(\"csv\", data_files={\"data\": \"../data/train_data_back.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rWnMjIDM2Ug"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CKPT = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLwnvBEaM9m0"
   },
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c80iN6O0MzV2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "\n",
    "def normalize(batch):\n",
    "    \"\"\"\n",
    "    This function should be used before tokenizing the input string.\n",
    "\n",
    "    Normalizes the input string in the following ways:\n",
    "    -> Converts from ş to ș, ţ to ț, etc.\n",
    "    -> Converts @mention to USER, #hashtag to HASHTAG, http... and www... to HTTPURL\n",
    "    -> Converts emoticons to :emoji_with_long_name:\n",
    "    -> Replaces :emoji_with_long_name: with emoji_with_long_name and replaces _, : and - with empty string\n",
    "    -> Removes multiple whitespaces with a single whitespace\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = batch[\"text\"]\n",
    "\n",
    "    # Make sure it's a string\n",
    "    sentence = str(sentence)\n",
    "\n",
    "    # Convert from ş to ș, ţ to ț, etc.\n",
    "    sentence = re.sub(r\"ş\", \"ș\", sentence)\n",
    "    sentence = re.sub(r\"Ş\", \"Ș\", sentence)\n",
    "    sentence = re.sub(r\"ţ\", \"ț\", sentence)\n",
    "    sentence = re.sub(r\"Ţ\", \"Ț\", sentence)\n",
    "\n",
    "    # Convert @mentions to USER, #hashtags to HASHTAG, http... and www... to HTTPURL\n",
    "    sentence = re.sub(r\"@\\S+\", \"USER\", sentence)\n",
    "    sentence = re.sub(r\"#\\S+\", \"HASHTAG\", sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \"HTTPURL\", sentence)\n",
    "    sentence = re.sub(r\"www\\S+\", \"HTTPURL\", sentence)\n",
    "\n",
    "    # Convert emoticons to :emoji_with_long_name:\n",
    "    sentence = emoji.demojize(sentence, delimiters=(\" :\", \": \"))\n",
    "\n",
    "    # Replace :emoji_with_long_name: with emojiwithlongname\n",
    "    sentence = re.sub(\n",
    "        r\":\\S+:\", lambda x: x.group(0).replace(\"_\", \"\").replace(\":\", \"\").replace(\"-\", \"\"), sentence\n",
    "    )\n",
    "\n",
    "    # Remove multiple whitespaces with a single whitespace\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "\n",
    "    return {\"text\": sentence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import concatenate_datasets, load_metric\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(\n",
    "        -1\n",
    "    )  # choose the predicted class (from an array of probabilites)\n",
    "\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bacc = balanced_accuracy_score(labels, preds)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"balanced_accuracy\": bacc}\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        # loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([26.315, 18.181, 1.265, 9.090, 200.0]).to(device))\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor([15.873, 11.111, 1.538, 5.555, 111.111]).to(device)\n",
    "        )  # [, , , , ]))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def train(index: int, dataset_tokenized):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_CKPT,\n",
    "        num_labels=5,\n",
    "        id2label=INT_TO_STR,\n",
    "        label2id=STR_TO_INT,\n",
    "        classifier_dropout=0.1,\n",
    "        # use_auth_token='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'\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"nitro-robertlarge-nlp-v1.9.{index}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_tokenized[\"train\"],\n",
    "        eval_dataset=dataset_tokenized[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csv\", data_files={\"data\": \"../data/train_data.csv\"})\n",
    "ds = ds.rename_column(\"Final Labels\", \"label\")\n",
    "ds = ds.rename_column(\"Text\", \"text\")\n",
    "ds = ds.remove_columns([\"Id\"])\n",
    "ds = ds[\"data\"]\n",
    "# ds = ds.class_encode_column('label')\n",
    "\n",
    "ds_back = ds_back.rename_column(\"text_back\", \"text\")\n",
    "ds_back = ds_back.rename_column(\"Final Labels\", \"label\")\n",
    "\n",
    "ds_back = ds_back.remove_columns([\"Text\", \"Id\"])\n",
    "ds_back = ds_back[\"data\"]\n",
    "# ds_back = ds_back.class_encode_column('label')\n",
    "\n",
    "ds_concat = concatenate_datasets([ds, ds_back])\n",
    "ds_concat = ds_concat.class_encode_column(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensamble_train(index=1, index_start=0, index_end=5):\n",
    "    seeds = []\n",
    "    for i in range(0, index):\n",
    "        seeds.append(seed + i)\n",
    "\n",
    "    for i in range(0, index):\n",
    "        if i >= index_start:\n",
    "            print(\"SEED: \", seed)\n",
    "            ds_split = ds_concat.train_test_split(\n",
    "                test_size=0.2, stratify_by_column=\"label\", seed=seeds[i]\n",
    "            )\n",
    "            ds_split = ds_split.map(lambda batch: normalize(batch), batched=False)\n",
    "            ds_tok_split = ds_split.map(\n",
    "                lambda batch: tokenize(batch), batched=True, batch_size=None\n",
    "            )\n",
    "            ds_tok_split.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "            train(i, ds_tok_split)\n",
    "        else:\n",
    "            ds_split = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\", seed=seed)\n",
    "\n",
    "\n",
    "ensamble_train(1, 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-offensive    29296 (~79%)\n",
    "offensive         4086 (~11%)\n",
    "direct            2048 (~5.5%)\n",
    "descriptive       1419 (~3,8%)\n",
    "reporting          208 (~0.5%) 0.79k1 = 0.005k2 = 1\n",
    "Total: 37,057\n",
    "\n",
    "0: 'descriptive',\n",
    "1: 'direct',\n",
    "2: 'non-offensive',\n",
    "3: 'offensive',\n",
    "4: 'reporting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnBOXVm7NPl2"
   },
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtGAl1ieQW-Y"
   },
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path: str, ds_tok):\n",
    "    model2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint_path,\n",
    "        num_labels=5,\n",
    "        id2label=INT_TO_STR,\n",
    "        label2id=STR_TO_INT,\n",
    "        classifier_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    training_args_ft = TrainingArguments(\n",
    "        output_dir=checkpoint_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer2 = CustomTrainer(\n",
    "        model=model2,\n",
    "        args=training_args_ft,\n",
    "        train_dataset=ds_tok,\n",
    "        eval_dataset=ds_tok,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjQ7z2-oMGsC"
   },
   "outputs": [],
   "source": [
    "ds_eval = load_dataset(\"csv\", data_files={\"data\": \"./dataset/test_data.csv\"})\n",
    "ds_eval = ds_eval.rename_column(\"Text\", \"text\")\n",
    "ds_eval = ds_eval.rename_column(\"Id\", \"id\")\n",
    "ds_eval_data = ds_eval[\"data\"]\n",
    "ds_eval_data = ds_eval_data.map(lambda batch: normalize(batch), batched=False)\n",
    "ds_eval_tok = ds_eval_data.map(lambda batch: tokenize(batch), batched=True, batch_size=None)\n",
    "ds_eval_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUOuL5xJNPl3"
   },
   "outputs": [],
   "source": [
    "ensamble_list = [\"./nitro-robertlarge-nlp-v1.9.0/checkpoint-3540\"]\n",
    "\n",
    "ensamble_predictions = []\n",
    "for model in ensamble_list:\n",
    "    trainer = load_model(model, ds_eval_tok)\n",
    "\n",
    "    predictions = trainer.predict(ds_eval_tok)\n",
    "    ensamble_predictions.append(predictions)\n",
    "\n",
    "    del trainer\n",
    "\n",
    "ensamble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ensamble_prediction = ensamble_predictions[0].predictions\n",
    "\n",
    "for i in range(1, len(ensamble_predictions)):\n",
    "    print(i)\n",
    "    final_ensamble_prediction = final_ensamble_prediction + ensamble_predictions[i].predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUPd_OSuNPl3"
   },
   "outputs": [],
   "source": [
    "preds = np.argmax(np.array(final_ensamble_prediction), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjNtmS1DNPl3"
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38XxQCZWNPl3"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({})\n",
    "\n",
    "with open(\"./subs/nitro-robertweet-nlp-v2.1.0.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    data = []\n",
    "    for i, pred in enumerate(preds):\n",
    "        data.append([i, INT_TO_STR[pred]])\n",
    "\n",
    "    header = [\"Id\", \"Label\"]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5f9qLKlNPl3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "efc8b837990332cc640cf04fb86e57a3faa233210bbd8a49efb74870320cb06c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
